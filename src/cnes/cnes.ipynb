{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613cbf49",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.9.21)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "from .utils import compression_for_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36447a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "barnabe-sil/dengue/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184975ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando a compressão com brotli...\n",
      "Arquivo /Users/silmara.barnabe/Downloads/mestrado/dengue/dengue_venv/data/_part_1.parquet gerado com tamanho: 26.84 MB\n",
      "Tamanho maior que 25.0MB. Ajustando a partição...\n",
      "Arquivo /Users/silmara.barnabe/Downloads/mestrado/dengue/dengue_venv/data/_part_2.parquet gerado com tamanho: 27.18 MB\n",
      "Tamanho maior que 25.0MB. Ajustando a partição...\n",
      "Arquivo /Users/silmara.barnabe/Downloads/mestrado/dengue/dengue_venv/data/_part_3.parquet gerado com tamanho: 0.03 MB\n",
      "Processo de partição concluído.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "# Carregar o arquivo CSV\n",
    "file_path = '/Users/silmara.barnabe/Downloads/mestrado/dengue/dengue_venv/data/'\n",
    "filename = 'tbEstabelecimento202503.csv'\n",
    "full_file_path = os.path.join(file_path, filename)\n",
    "\n",
    "df = pd.read_csv(full_file_path, delimiter=';', dtype=str, encoding='ISO-8859-1')\n",
    "\n",
    "# Definir o tamanho máximo dos arquivos em MB\n",
    "max_size_mb = 25  # Tamanho máximo de 25MB para cada arquivo Parquet\n",
    "\n",
    "# Tentar diferentes compressões\n",
    "compressions = ['brotli']\n",
    "for compression in compressions:\n",
    "    print(f\"\\nIniciando a compressão com {compression}...\")\n",
    "    save_parquet_in_chunks(df, file_path, max_size_mb, compression=compression)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9fedfe",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "# Exemplo de uso:\n",
    "\n",
    "## Carregar o arquivo CSV\n",
    "file_path = 'seu_arquivo.csv'\n",
    "df = pd.read_csv(file_path, delimiter=';', dtype=str, encoding='ISO-8859-1')\n",
    "\n",
    "## Tentar diferentes compressões\n",
    "compressions = ['snappy', 'gzip', 'brotli', 'lz4']\n",
    "for compression in compressions:\n",
    "    print(f\"\\nIniciando a compressão com {compression}...\")\n",
    "    save_parquet_in_chunks(df, 'seu_arquivo', compression=compression)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f2fdc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Definir o caminho do diretório e o nome do arquivo\n",
    "file_path = '/Users/silmara.barnabe/Downloads/mestrado/dengue/dengue_venv/data/'\n",
    "file_name = 'tbEstabelecimento202503.csv'  # Nome correto do arquivo CSV\n",
    "\n",
    "# Criar o caminho completo para o arquivo\n",
    "full_file_path = os.path.join(file_path, file_name)\n",
    "\n",
    "# Carregar o arquivo CSV\n",
    "df = pd.read_csv(full_file_path, delimiter=';', dtype=str, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72574797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 567123 entries, 0 to 567122\n",
      "Data columns (total 56 columns):\n",
      " #   Column                                       Non-Null Count   Dtype \n",
      "---  ------                                       --------------   ----- \n",
      " 0   CO_UNIDADE                                   567123 non-null  object\n",
      " 1   CO_CNES                                      567123 non-null  object\n",
      " 2   NU_CNPJ_MANTENEDORA                          115370 non-null  object\n",
      " 3   TP_PFPJ                                      567123 non-null  object\n",
      " 4   NIVEL_DEP                                    567123 non-null  object\n",
      " 5   NO_RAZAO_SOCIAL                              567122 non-null  object\n",
      " 6   NO_FANTASIA                                  567033 non-null  object\n",
      " 7   NO_LOGRADOURO                                567122 non-null  object\n",
      " 8   NU_ENDERECO                                  566808 non-null  object\n",
      " 9   NO_COMPLEMENTO                               274726 non-null  object\n",
      " 10  NO_BAIRRO                                    567121 non-null  object\n",
      " 11  CO_CEP                                       567123 non-null  object\n",
      " 12  CO_REGIAO_SAUDE                              239572 non-null  object\n",
      " 13  CO_MICRO_REGIAO                              0 non-null       object\n",
      " 14  CO_DISTRITO_SANITARIO                        40707 non-null   object\n",
      " 15  CO_DISTRITO_ADMINISTRATIVO                   1070 non-null    object\n",
      " 16  NU_TELEFONE                                  439498 non-null  object\n",
      " 17  NU_FAX                                       0 non-null       object\n",
      " 18  NO_EMAIL                                     310597 non-null  object\n",
      " 19  NU_CPF                                       163421 non-null  object\n",
      " 20  NU_CNPJ                                      291154 non-null  object\n",
      " 21  CO_ATIVIDADE                                 567123 non-null  object\n",
      " 22  CO_CLIENTELA                                 559047 non-null  object\n",
      " 23  NU_ALVARA                                    384119 non-null  object\n",
      " 24  DT_EXPEDICAO                                 383819 non-null  object\n",
      " 25  TP_ORGAO_EXPEDIDOR                           388941 non-null  object\n",
      " 26  DT_VAL_LIC_SANI                              232100 non-null  object\n",
      " 27  TP_LIC_SANI                                  232475 non-null  object\n",
      " 28  TP_UNIDADE                                   567123 non-null  object\n",
      " 29  CO_TURNO_ATENDIMENTO                         565374 non-null  object\n",
      " 30  CO_ESTADO_GESTOR                             567123 non-null  object\n",
      " 31  CO_MUNICIPIO_GESTOR                          567123 non-null  object\n",
      " 32  TO_CHAR(DT_ATUALIZACAO,'DD/MM/YYYY')         567123 non-null  object\n",
      " 33  CO_USUARIO                                   567123 non-null  object\n",
      " 34  CO_CPFDIRETORCLN                             523308 non-null  object\n",
      " 35  REG_DIRETORCLN                               330044 non-null  object\n",
      " 36  ST_ADESAO_FILANTROP                          2608 non-null    object\n",
      " 37  CO_MOTIVO_DESAB                              114494 non-null  object\n",
      " 38  NO_URL                                       11036 non-null   object\n",
      " 39  NU_LATITUDE                                  507208 non-null  object\n",
      " 40  NU_LONGITUDE                                 507256 non-null  object\n",
      " 41  TO_CHAR(DT_ATU_GEO,'DD/MM/YYYY')             447517 non-null  object\n",
      " 42  NO_USUARIO_GEO                               446897 non-null  object\n",
      " 43  CO_NATUREZA_JUR                              567119 non-null  object\n",
      " 44  TP_ESTAB_SEMPRE_ABERTO                       519083 non-null  object\n",
      " 45  ST_GERACREDITO_GERENTE_SGIF                  90 non-null      object\n",
      " 46  ST_CONEXAO_INTERNET                          530132 non-null  object\n",
      " 47  CO_TIPO_UNIDADE                              0 non-null       object\n",
      " 48  NO_FANTASIA_ABREV                            0 non-null       object\n",
      " 49  TP_GESTAO                                    567123 non-null  object\n",
      " 50  TO_CHAR(DT_ATUALIZACAO_ORIGEM,'DD/MM/YYYY')  567123 non-null  object\n",
      " 51  CO_TIPO_ESTABELECIMENTO                      523800 non-null  object\n",
      " 52  CO_ATIVIDADE_PRINCIPAL                       523800 non-null  object\n",
      " 53  ST_CONTRATO_FORMALIZADO                      235847 non-null  object\n",
      " 54  CO_TIPO_ABRANGENCIA                          61054 non-null   object\n",
      " 55  ST_COWORKING                                 199635 non-null  object\n",
      "dtypes: object(56)\n",
      "memory usage: 242.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e6e5188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando a compressão com brotli...\n",
      "Arquivo /Users/silmara.barnabe/Downloads/mestrado/dengue/dengue_venv/data/_part_1.parquet gerado com tamanho: 26.84 MB\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m compression \u001b[38;5;129;01min\u001b[39;00m compressions:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIniciando a compressão com \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompression\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43msave_parquet_in_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtbEstabelecimento202503\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36msave_parquet_in_chunks\u001b[39m\u001b[34m(df, base_filename, max_size_mb, compression)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mArquivo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m gerado com tamanho: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Ajustar a divisão caso o arquivo gerado seja maior que 25MB\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfile_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_size_mb\u001b[49m:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTamanho maior que \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_size_mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mMB. Ajustando a partição...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m     chunk_df = df.iloc[start_index:end_index - (chunk_size // \u001b[32m4\u001b[39m)]  \u001b[38;5;66;03m# Dividir ainda mais\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: '>' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "# Tentar diferentes compressões\n",
    "compressions = ['brotli']\n",
    "for compression in compressions:\n",
    "    print(f\"\\nIniciando a compressão com {compression}...\")\n",
    "    save_parquet_in_chunks(df, file_path, 'tbEstabelecimento202503', compression=compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8827cbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m filename = \u001b[33m'\u001b[39m\u001b[33mtbEstabelecimento202503.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     65\u001b[39m full_file_path = os.path.join(file_path, filename)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mISO-8859-1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Tentar diferentes compressões\u001b[39;00m\n\u001b[32m     70\u001b[39m compressions = [\u001b[33m'\u001b[39m\u001b[33mbrotli\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mestrado/dengue/dengue_venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mestrado/dengue/dengue_venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mestrado/dengue/dengue_venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1964\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1958\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m col_dict.items():\n\u001b[32m   1959\u001b[39m         d = (\n\u001b[32m   1960\u001b[39m             dtype[k]\n\u001b[32m   1961\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m pandas_dtype(dtype[k]) \u001b[38;5;129;01min\u001b[39;00m (np.str_, np.object_)\n\u001b[32m   1962\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1963\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m         new_col_dict[k] = \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1965\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1966\u001b[39m     new_col_dict = col_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mestrado/dengue/dengue_venv/lib/python3.13/site-packages/pandas/core/series.py:584\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    582\u001b[39m         data = data.copy()\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     data = \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     manager = _get_option(\u001b[33m\"\u001b[39m\u001b[33mmode.data_manager\u001b[39m\u001b[33m\"\u001b[39m, silent=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    587\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mestrado/dengue/dengue_venv/lib/python3.13/site-packages/pandas/core/construction.py:625\u001b[39m, in \u001b[36msanitize_array\u001b[39m\u001b[34m(data, index, dtype, copy, allow_2d)\u001b[39m\n\u001b[32m    621\u001b[39m             subarr = subarr.copy()\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    624\u001b[39m         \u001b[38;5;66;03m# we will try to copy by-definition here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m         subarr = \u001b[43m_try_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[33m\"\u001b[39m\u001b[33m__array__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# e.g. dask array GH#38645\u001b[39;00m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mestrado/dengue/dengue_venv/lib/python3.13/site-packages/pandas/core/construction.py:806\u001b[39m, in \u001b[36m_try_cast\u001b[39m\u001b[34m(arr, dtype, copy)\u001b[39m\n\u001b[32m    804\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    805\u001b[39m         shape = (\u001b[38;5;28mlen\u001b[39m(arr),)\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mensure_string_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_na_value\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m.reshape(\n\u001b[32m    807\u001b[39m         shape\n\u001b[32m    808\u001b[39m     )\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmM\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m maybe_cast_to_datetime(arr, dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Função para salvar o DataFrame em arquivos Parquet de tamanho máximo de 25MB\n",
    "def save_parquet_in_chunks(df, base_filename, max_size_mb=25, compression='snappy'):\n",
    "    \"\"\"\n",
    "    Salva um DataFrame em múltiplos arquivos Parquet de tamanho máximo especificado (em MB).\n",
    "    \n",
    "    Parâmetros:\n",
    "    df (DataFrame): O DataFrame que será dividido e salvo.\n",
    "    base_filename (str): O nome base para os arquivos Parquet gerados.\n",
    "    max_size_mb (int ou float): O tamanho máximo de cada arquivo Parquet em MB (padrão é 25MB).\n",
    "    compression (str): O tipo de compressão a ser utilizado (pode ser 'snappy', 'gzip', 'brotli', 'lz4').\n",
    "\n",
    "    Retorna:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Garantir que max_size_mb seja um número\n",
    "    max_size_mb = float(max_size_mb)\n",
    "    \n",
    "    # Criar uma pasta para a compressão, caso não exista\n",
    "    output_dir = os.path.join(base_filename, compression)\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Cria a pasta se não existir\n",
    "\n",
    "    # Inicializando o índice para a partição\n",
    "    partition_index = 0\n",
    "    start_index = 0\n",
    "    chunk_size = len(df)  # Tamanho do DataFrame inteiro\n",
    "    total_size = df.memory_usage(deep=True).sum() / (1024 * 1024)  # Tamanho em MB\n",
    "\n",
    "    # Salvar as divisões como arquivos Parquet\n",
    "    while start_index < chunk_size:\n",
    "        partition_index += 1\n",
    "        # Divida o DataFrame em partes com base no número de linhas\n",
    "        end_index = start_index + (chunk_size // 2)  # Inicialmente dividir em metades\n",
    "        \n",
    "        # Verificar o tamanho do arquivo Parquet\n",
    "        chunk_df = df.iloc[start_index:end_index]  # Subconjunto do DataFrame\n",
    "        \n",
    "        chunk_filename = os.path.join(output_dir, f\"{base_filename}_part_{partition_index}.parquet\")\n",
    "        \n",
    "        # Salvar em Parquet com a compressão desejada\n",
    "        chunk_df.to_parquet(chunk_filename, compression=compression)\n",
    "        \n",
    "        # Verificar o tamanho do arquivo gerado\n",
    "        file_size = os.path.getsize(chunk_filename) / (1024 * 1024)  # Tamanho em MB\n",
    "        print(f\"Arquivo {chunk_filename} gerado com tamanho: {file_size:.2f} MB\")\n",
    "        \n",
    "        # Ajustar a divisão caso o arquivo gerado seja maior que 25MB\n",
    "        if file_size > max_size_mb:\n",
    "            print(f\"Tamanho maior que {max_size_mb}MB. Ajustando a partição...\")\n",
    "            chunk_df = df.iloc[start_index:end_index - (chunk_size // 4)]  # Dividir ainda mais\n",
    "            chunk_df.to_parquet(chunk_filename, compression=compression)\n",
    "\n",
    "        # Atualizar o índice para a próxima parte\n",
    "        start_index = end_index\n",
    "\n",
    "    print(\"Processo de partição concluído.\")\n",
    "\n",
    "# Carregar o arquivo CSV\n",
    "file_path = '/Users/silmara.barnabe/Downloads/mestrado/dengue/dengue_venv/data/'\n",
    "filename = 'tbEstabelecimento202503.csv'\n",
    "full_file_path = os.path.join(file_path, filename)\n",
    "\n",
    "df = pd.read_csv(full_file_path, delimiter=';', dtype=str, encoding='ISO-8859-1')\n",
    "\n",
    "# Tentar diferentes compressões\n",
    "compressions = ['brotli']\n",
    "for compression in compressions:\n",
    "    print(f\"\\nIniciando a compressão com {compression}...\")\n",
    "    save_parquet_in_chunks(df, file_path, 'tbEstabelecimento202503', compression=compression)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
